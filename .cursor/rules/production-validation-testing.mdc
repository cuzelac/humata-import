# Production Validation and Testing Rule

## Description
This rule establishes comprehensive production validation and testing practices, particularly for critical systems like duplicate detection, based on lessons learned from implementing the Humata Import duplicate detection system.

## Rule
When preparing systems for production deployment:

1. **Production Validation Requirements**
   - MUST create comprehensive validation scripts that test all critical functionality
   - MUST validate performance targets with real production-like data
   - MUST test error handling and edge cases thoroughly
   - MUST verify data integrity and consistency across all operations

2. **Validation Categories**
   - **Core System Validation**: Essential functionality and feature completeness
   - **Performance Validation**: Response times, throughput, and scalability
   - **Data Integrity Validation**: Data consistency, accuracy, and reliability
   - **Error Handling Validation**: Robust error management and recovery
   - **Integration Validation**: End-to-end workflow functionality

3. **Validation Scripts**
   - MUST be automated and repeatable
   - MUST provide clear pass/fail results with detailed explanations
   - MUST include performance benchmarks and targets
   - MUST generate comprehensive reports for stakeholders

4. **Production Readiness Assessment**
   - MUST achieve 100% validation pass rate for critical systems
   - MUST meet all performance targets consistently
   - MUST demonstrate error handling robustness
   - MUST provide clear go/no-go decision criteria

## Implementation Guidelines

### Validation Script Structure
```ruby
class ProductionValidator
  def initialize(db_path)
    @db_path = db_path
    @validation_results = []
  end

  def run
    validate_core_system
    validate_performance
    validate_data_integrity
    validate_error_handling
    assess_production_readiness
  end

  private

  def validate_core_system
    # Test essential functionality
    # Record results in @validation_results
  end

  def assess_production_readiness
    # Calculate overall readiness score
    # Provide clear go/no-go decision
  end
end
```

### Validation Categories

#### Core System Validation
```ruby
def validate_core_system
  # Test 1: Feature completeness
  total_files = @db.get_first_value("SELECT COUNT(*) FROM file_records")
  files_with_hash = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE file_hash IS NOT NULL")
  
  if total_files == files_with_hash
    @validation_results << { 
      component: 'File Hash Population', 
      status: 'PASS', 
      details: "#{total_files} files processed" 
    }
  else
    @validation_results << { 
      component: 'File Hash Population', 
      status: 'FAIL', 
      details: "Missing hashes for #{total_files - files_with_hash} files" 
    }
  end
  
  # Test 2: Core functionality
  duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
  if duplicates.any?
    @validation_results << { 
      component: 'Duplicate Detection', 
      status: 'PASS', 
      details: "#{duplicates.size} duplicate groups identified" 
    }
  end
end
```

#### Performance Validation
```ruby
def validate_performance
  # Test 1: Duplicate detection performance
  start_time = Time.now
  duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
  end_time = Time.now
  detection_time = end_time - start_time
  
  if detection_time < 0.1
    @validation_results << { 
      component: 'Detection Performance', 
      status: 'PASS', 
      details: "#{detection_time.round(3)}s for #{duplicates.size} groups" 
    }
  elsif detection_time < 1.0
    @validation_results << { 
      component: 'Detection Performance', 
      status: 'PASS', 
      details: "#{detection_time.round(3)}s for #{duplicates.size} groups" 
    }
  else
    @validation_results << { 
      component: 'Detection Performance', 
      status: 'WARNING', 
      details: "#{detection_time.round(3)}s for #{duplicates.size} groups" 
    }
  end
end
```

#### Data Integrity Validation
```ruby
def validate_data_integrity
  # Test 1: File hash validity
  invalid_hashes = @db.get_first_value(<<-SQL)
    SELECT COUNT(*) FROM file_records 
    WHERE file_hash IS NOT NULL 
    AND length(file_hash) != 32
  SQL
  
  if invalid_hashes == 0
    @validation_results << { 
      component: 'File Hash Validity', 
      status: 'PASS', 
      details: 'All hashes are valid MD5 format' 
    }
  else
    @validation_results << { 
      component: 'File Hash Validity', 
      status: 'FAIL', 
      details: "#{invalid_hashes} invalid hashes found" 
    }
  end
  
  # Test 2: Duplicate consistency
  duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
  inconsistent_duplicates = 0
  
  duplicates.each do |group|
    hash = group[:file_hash]
    count = group[:count]
    actual_count = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE file_hash = ?", [hash])
    inconsistent_duplicates += 1 if actual_count != count
  end
  
  if inconsistent_duplicates == 0
    @validation_results << { 
      component: 'Duplicate Consistency', 
      status: 'PASS', 
      details: 'All duplicate counts are accurate' 
    }
  else
    @validation_results << { 
      component: 'Duplicate Consistency', 
      status: 'FAIL', 
      details: "#{inconsistent_duplicates} count inconsistencies found" 
    }
  end
end
```

#### Error Handling Validation
```ruby
def validate_error_handling
  # Test 1: Nil file hash handling
  begin
    result = HumataImport::FileRecord.find_duplicate(@db, nil, 'test_file')
    if !result[:duplicate_found]
      @validation_results << { 
        component: 'Nil Hash Handling', 
        status: 'PASS', 
        details: 'Properly handles nil file hashes' 
      }
    else
      @validation_results << { 
        component: 'Nil Hash Handling', 
        status: 'FAIL', 
        details: 'Returns false positive for nil hash' 
      }
    end
  rescue => e
    @validation_results << { 
      component: 'Nil Hash Handling', 
      status: 'ERROR', 
      details: e.message 
    }
  end
end
```

### Production Readiness Assessment
```ruby
def assess_production_readiness
  passed = @validation_results.count { |r| r[:status] == 'PASS' }
  warnings = @validation_results.count { |r| r[:status] == 'WARNING' }
  failed = @validation_results.count { |r| r[:status] == 'FAIL' }
  errors = @validation_results.count { |r| r[:status] == 'ERROR' }
  
  puts "\nðŸŽ¯ Production Readiness Assessment"
  puts "   Validation Results:"
  puts "   âœ… Passed: #{passed}"
  puts "   âš ï¸  Warnings: #{warnings}"
  puts "   âŒ Failed: #{failed}"
  puts "   ðŸš¨ Errors: #{errors}"
  
  if failed == 0 && errors == 0
    if warnings == 0
      puts "   ðŸŽ‰ READY FOR PRODUCTION - All validations passed"
    else
      puts "   âœ… READY FOR PRODUCTION - All critical validations passed"
    end
  elsif failed == 0
    puts "   âš ï¸  CONDITIONALLY READY - All critical validations passed"
  else
    puts "   âŒ NOT READY FOR PRODUCTION - Critical validations failed"
  end
end
```

## Examples

### âœ… Good: Comprehensive Production Validation
```ruby
# scripts/production_validation.rb
class ProductionValidator
  def run
    validate_core_system
    validate_performance
    validate_data_integrity
    validate_error_handling
    assess_production_readiness
  end
  
  private
  
  def validate_core_system
    # Test all essential functionality
    # Record detailed results
  end
  
  def validate_performance
    # Test performance targets
    # Use real data for accurate results
  end
  
  def validate_data_integrity
    # Verify data consistency
    # Check for corruption or inconsistencies
  end
  
  def validate_error_handling
    # Test edge cases and error conditions
    # Ensure robust error management
  end
end
```

### âŒ Bad: Insufficient Validation
```ruby
# Missing validation script
# No performance testing
# No error handling validation
# No production readiness assessment
```

## Validation Requirements

### Performance Targets
- **Critical Operations**: Must complete within specified time limits
- **Scalability**: Must maintain performance with increasing data size
- **Memory Usage**: Must remain within acceptable bounds
- **Response Time**: Must meet user experience requirements

### Data Integrity Requirements
- **Consistency**: All data must be internally consistent
- **Accuracy**: Calculations and counts must be correct
- **Completeness**: Required data must be present
- **Validity**: Data must conform to expected formats

### Error Handling Requirements
- **Graceful Degradation**: System must handle errors without crashing
- **Informative Messages**: Error messages must be helpful
- **Recovery**: System must recover from error conditions
- **Logging**: All errors must be properly logged

## Benefits
- **Confidence**: Clear understanding of system readiness
- **Quality**: Comprehensive validation of all critical functionality
- **Risk Mitigation**: Identifies issues before production deployment
- **Documentation**: Clear record of validation results and decisions

## Enforcement
- All production deployments must include comprehensive validation
- Validation scripts must be automated and repeatable
- Performance targets must be met consistently
- All critical validations must pass before production deployment

## Notes
- This rule complements `test-performance.mdc` and `test-isolation.mdc`
- Based on successful production validation of duplicate detection system
- Essential for maintaining high-quality production deployments
- Provides clear criteria for go/no-go production decisions
description:
globs:
alwaysApply: true
---
