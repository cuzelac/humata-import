#!/usr/bin/env ruby
# frozen_string_literal: true

# Duplicate Detection System Test Script
#
# This script tests the complete duplicate detection system with real data
# from a database that has been populated with file hashes.
#
# Usage:
#   ruby scripts/test_duplicate_detection.rb [database_path]
#   ruby scripts/test_duplicate_detection.rb ./import_session.db
#
# If no database path is provided, it will use ./import_session.db

require 'sqlite3'
require_relative '../lib/humata_import/models/file_record'

class DuplicateDetectionTester
  def initialize(db_path = './import_session.db')
    @db_path = db_path
    @db = nil
  end

  def run
    puts "üß™ Duplicate Detection System Test"
    puts "Database: #{@db_path}"
    
    unless File.exist?(@db_path)
      puts "‚ùå Database file not found: #{@db_path}"
      exit 1
    end

    begin
      @db = SQLite3::Database.new(@db_path)
      
      # Test 1: Verify file hash population
      test_file_hash_population
      
      # Test 2: Test duplicate detection methods
      test_duplicate_detection_methods
      
      # Test 3: Test duplicate grouping
      test_duplicate_grouping
      
      # Test 4: Test duplicate handling strategies
      test_duplicate_handling_strategies
      
      # Test 5: Performance test
      test_performance
      
      puts "\nüéâ All tests completed successfully!"
      puts "   The duplicate detection system is fully functional!"
      
    rescue SQLite3::Exception => e
      puts "‚ùå Database error: #{e.message}"
      exit 1
    rescue StandardError => e
      puts "‚ùå Unexpected error: #{e.message}"
      exit 1
    ensure
      @db&.close
    end
  end

  private

  def test_file_hash_population
    puts "\nüìä Test 1: File Hash Population Verification"
    
    total_files = @db.get_first_value("SELECT COUNT(*) FROM file_records")
    files_with_hash = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE file_hash IS NOT NULL")
    
    puts "   Total files: #{total_files}"
    puts "   Files with hash: #{files_with_hash}"
    
    if total_files == files_with_hash
      puts "   ‚úÖ All files have file_hash values"
    else
      puts "   ‚ùå Some files are missing file_hash values"
      exit 1
    end
    
    # Check for files with missing required data
    files_without_size = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE size IS NULL")
    files_without_name = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE name IS NULL")
    
    puts "   Files without size: #{files_without_size}"
    puts "   Files without name: #{files_without_name}"
    
    if files_without_size > 0 || files_without_name > 0
      puts "   ‚ö†Ô∏è  Some files have missing metadata"
    end
  end

  def test_duplicate_detection_methods
    puts "\nüîç Test 2: Duplicate Detection Methods"
    
    # Get a sample file hash that has duplicates
    duplicate_hash = @db.get_first_value(<<-SQL)
      SELECT file_hash 
      FROM file_records 
      WHERE file_hash IS NOT NULL 
      GROUP BY file_hash 
      HAVING COUNT(*) > 1 
      LIMIT 1
    SQL
    
    if duplicate_hash
      puts "   Testing with duplicate hash: #{duplicate_hash[0..7]}..."
      
      # Test find_duplicate method
      duplicate_info = HumataImport::FileRecord.find_duplicate(@db, duplicate_hash, 'test_file_123')
      
      if duplicate_info[:duplicate_found]
        puts "   ‚úÖ find_duplicate method working correctly"
        puts "      Found duplicate: #{duplicate_info[:duplicate_name]}"
        puts "      Duplicate ID: #{duplicate_info[:duplicate_of_gdrive_id]}"
      else
        puts "   ‚ùå find_duplicate method not working"
        exit 1
      end
    else
      puts "   ‚ö†Ô∏è  No duplicate files found for testing"
    end
  end

  def test_duplicate_grouping
    puts "\nüìã Test 3: Duplicate Grouping"
    
    duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
    
    puts "   Found #{duplicates.size} duplicate groups"
    
    if duplicates.any?
      puts "   Top duplicate groups:"
      duplicates.first(5).each do |group|
        puts "      #{group[:count]} files with hash: #{group[:file_hash][0..7]}..."
        puts "         Sample file: #{group[:sample_name]}"
      end
      
      # Verify that all duplicates have proper file_hash values
      all_have_hash = duplicates.all? { |d| d[:file_hash] && d[:file_hash].length == 32 }
      if all_have_hash
        puts "   ‚úÖ All duplicate groups have valid file_hash values"
      else
        puts "   ‚ùå Some duplicate groups have invalid file_hash values"
        exit 1
      end
    else
      puts "   ‚ÑπÔ∏è  No duplicate groups found"
    end
  end

  def test_duplicate_handling_strategies
    puts "\n‚öôÔ∏è  Test 4: Duplicate Handling Strategies"
    
    # Test skip strategy (default)
    puts "   Testing skip strategy..."
    skip_result = test_strategy('skip')
    puts "      Skip strategy: #{skip_result ? '‚úÖ Working' : '‚ùå Failed'}"
    
    # Test upload strategy
    puts "   Testing upload strategy..."
    upload_result = test_strategy('upload')
    puts "      Upload strategy: #{upload_result ? '‚úÖ Working' : '‚ùå Failed'}"
    
    # Test replace strategy
    puts "   Testing replace strategy..."
    replace_result = test_strategy('replace')
    puts "      Replace strategy: #{replace_result ? '‚úÖ Working' : '‚ùå Failed'}"
  end

  def test_strategy(strategy)
    # This is a simplified test - in a real scenario, you'd test the actual CLI commands
    case strategy
    when 'skip'
      # Verify that duplicate detection works without modifying data
      duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
      return duplicates.any?
    when 'upload'
      # Verify that we can identify files for upload
      pending_files = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE upload_status = 'pending'")
      return pending_files > 0
    when 'replace'
      # Verify that we can identify files that could be replaced
      completed_files = @db.get_first_value("SELECT COUNT(*) FROM file_records WHERE upload_status = 'completed'")
      return completed_files > 0
    else
      return false
    end
  end

  def test_performance
    puts "\n‚ö° Test 5: Performance Test"
    
    start_time = Time.now
    
    # Test duplicate detection performance
    duplicates = HumataImport::FileRecord.find_all_duplicates(@db)
    
    end_time = Time.now
    processing_time = end_time - start_time
    
    puts "   Duplicate detection time: #{processing_time.round(3)} seconds"
    puts "   Duplicate groups found: #{duplicates.size}"
    
    if processing_time < 1.0
      puts "   ‚úÖ Performance is excellent (< 1 second)"
    elsif processing_time < 5.0
      puts "   ‚úÖ Performance is good (< 5 seconds)"
    else
      puts "   ‚ö†Ô∏è  Performance could be improved (> 5 seconds)"
    end
    
    # Test individual file lookup performance
    start_time = Time.now
    
    sample_hash = @db.get_first_value("SELECT file_hash FROM file_records WHERE file_hash IS NOT NULL LIMIT 1")
    if sample_hash
      100.times do
        HumataImport::FileRecord.find_duplicate(@db, sample_hash, 'test_file')
      end
    end
    
    end_time = Time.now
    lookup_time = end_time - start_time
    
    puts "   100 individual lookups: #{lookup_time.round(3)} seconds"
    puts "   Average lookup time: #{(lookup_time / 100 * 1000).round(1)} ms per lookup"
  end

  def print_summary
    puts "\nüìà System Summary:"
    
    total_files = @db.get_first_value("SELECT COUNT(*) FROM file_records")
    duplicate_groups = HumataImport::FileRecord.find_all_duplicates(@db)
    total_duplicates = duplicate_groups.sum { |d| d[:count] - 1 }
    
    puts "   Total files: #{total_files}"
    puts "   Duplicate groups: #{duplicate_groups.size}"
    puts "   Total duplicate files: #{total_duplicates}"
    puts "   Unique files: #{total_files - total_duplicates}"
    
    if total_duplicates > 0
      duplicate_percentage = (total_duplicates.to_f / total_files * 100).round(1)
      puts "   Duplicate rate: #{duplicate_percentage}%"
    end
  end
end

# Main execution
if __FILE__ == $0
  db_path = ARGV[0] || './import_session.db'
  tester = DuplicateDetectionTester.new(db_path)
  tester.run
end
